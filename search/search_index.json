{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"notebook/nlp/","title":"Natural Language Processing","text":"In\u00a0[1]: hide-cell Copied! <pre># Install the necessary dependencies\n\nimport os\nimport sys\n!{sys.executable} -m pip install --quiet pandas scikit-learn numpy matplotlib jupyterlab_myst ipython\n</pre> # Install the necessary dependencies  import os import sys !{sys.executable} -m pip install --quiet pandas scikit-learn numpy matplotlib jupyterlab_myst ipython <p>license: code: MIT content: CC-BY-4.0 github: https://github.com/ocademy-ai/machine-learning venue: By Ocademy open_access: true bibliography:</p> <ul> <li>https://raw.githubusercontent.com/ocademy-ai/machine-learning/main/open-machine-learning-jupyter-book/references.bib</li> </ul> <p>Natural Language Processing (NLP) stands as a pivotal technology in the realm of artificial intelligence, bridging the gap between human communication and computer understanding. It is a multidisciplinary domain that empowers computers to interpret, analyze, and generate human language, enabling seamless interaction between humans and machines. The significance of NLP is evident in its widespread applications, ranging from automated customer support to real-time language translation.</p> <p>This section aims to provide newcomers with a comprehensive overview of NLP, its workings, applications, challenges, and future outlook.</p>  Image: Natural Language Processing <p>Natural Language Processing (NLP) is a branch of artificial intelligence that focuses on the interaction between computers and humans through natural language. The objective is to program computers to process and analyze large amounts of natural language data.</p> <p>NLP involves enabling machines to understand, interpret, and produce human language in a way that is both valuable and meaningful. OpenAI, known for developing advanced language models like ChatGPT, highlights the importance of NLP in creating intelligent systems that can understand, respond to, and generate text, making technology more user-friendly and accessible</p> <p>Natural Language Processing is not a monolithic, singular approach, but rather, it is composed of several components, each contributing to the overall understanding of language. The main components that NLP strives to understand are Syntax, Semantics, Pragmatics, and Discourse.</p>  Image: Semantics in NLP <p>Understanding these components is crucial for anyone delving into NLP, as they form the backbone of how NLP models interpret and generate human langua</p> <p>Natural Language Processing has found extensive applications across various industries, revolutionizing the way businesses operate and interact with users. Here are some of the key industry applications of NLP.</p> <p>The applications of NLP are diverse and pervasive, impacting various industries and our daily interactions with technology. Understanding these applications provides a glimpse into the transformative potential of NLP in shaping the future of technology and human interaction.</p> <p>Natural Language Processing, despite its advancements, faces several challenges due to the inherent complexities and nuances of human language. Here are some of the challenges in NLP:</p> <ul> <li>Ambiguity. Human language is often ambiguous, with words having multiple meanings, making it challenging for NLP models to interpret the correct meaning in different contexts.</li> <li>Context. Understanding the context in which words are used is crucial for accurate interpretation, and it remains a significant challenge for NLP.</li> <li>Sarcasm and irony. Detecting sarcasm and irony is particularly challenging as it requires understanding the intended meaning, which may be opposite to the literal meaning.</li> <li>Cultural nuances. Language is deeply intertwined with culture, and understanding cultural nuances and idioms is essential for effective NLP.</li> </ul> <p>Now that we have a preliminary understanding of natural language processing, let\u2019s train a model about disaster tweets to help you better understand.</p> <p>First, let's import the necessary libraries.</p> In\u00a0[1]: Copied! <pre>import numpy as np\nimport pandas as pd\nfrom sklearn import feature_extraction, linear_model, model_selection, preprocessing\n</pre> import numpy as np import pandas as pd from sklearn import feature_extraction, linear_model, model_selection, preprocessing <p>Let's now import the dataset which contains numerous tweet texts. Each tweet is labeled as either related to a real disaster or not. Our task is to utilize Natural Language Processing (NLP) techniques to process and analyze these tweet texts. We aim to build a model that can automatically identify whether a tweet is related to a disaster or not.</p> In\u00a0[2]: Copied! <pre>train_df = pd.read_csv(\"https://static-1300131294.cos.ap-shanghai.myqcloud.com/data/deep-learning/nlp/disaster_tweets_train.csv\")\ntest_df = pd.read_csv(\"https://static-1300131294.cos.ap-shanghai.myqcloud.com/data/deep-learning/nlp/disaster_tweets_test.csv\")\n</pre> train_df = pd.read_csv(\"https://static-1300131294.cos.ap-shanghai.myqcloud.com/data/deep-learning/nlp/disaster_tweets_train.csv\") test_df = pd.read_csv(\"https://static-1300131294.cos.ap-shanghai.myqcloud.com/data/deep-learning/nlp/disaster_tweets_test.csv\") <p>Let's take a quick look at our data... first, an example of what is NOT a disaster tweet.</p> In\u00a0[5]: Copied! <pre>train_df[train_df[\"target\"] == 0][\"text\"].values[1]\n</pre> train_df[train_df[\"target\"] == 0][\"text\"].values[1] Out[5]: <pre>'I love fruits'</pre> <p>Then an example of what is a disaster tweet.</p> In\u00a0[6]: Copied! <pre>train_df[train_df[\"target\"] == 1][\"text\"].values[1]\n</pre> train_df[train_df[\"target\"] == 1][\"text\"].values[1] Out[6]: <pre>'Forest fire near La Ronge Sask. Canada'</pre> In\u00a0[15]: Copied! <pre>count_vectorizer = feature_extraction.text.CountVectorizer()\n\n## let's get counts for the first 5 tweets in the data\nexample_train_vectors = count_vectorizer.fit_transform(train_df[\"text\"][0:5])\n</pre> count_vectorizer = feature_extraction.text.CountVectorizer()  ## let's get counts for the first 5 tweets in the data example_train_vectors = count_vectorizer.fit_transform(train_df[\"text\"][0:5]) In\u00a0[19]: Copied! <pre>train_df[\"text\"][0]\n</pre> train_df[\"text\"][0] Out[19]: <pre>'Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all'</pre> In\u00a0[18]: Copied! <pre>## we use .todense() here because these vectors are \"sparse\" (only non-zero elements are kept to save space)\nprint(example_train_vectors[0].todense().shape)\nprint(example_train_vectors[0].todense())\n</pre> ## we use .todense() here because these vectors are \"sparse\" (only non-zero elements are kept to save space) print(example_train_vectors[0].todense().shape) print(example_train_vectors[0].todense()) <pre>(1, 54)\n[[0 0 0 1 1 1 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0\n  0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 1 0]]\n</pre> <p>The above tells us that:</p> <ul> <li>There are 54 unique words (or \"tokens\") in the first five tweets.</li> <li>The first tweet contains only some of those unique tokens - all of the non-zero counts above are the tokens that exist in the first tweet.</li> </ul> <p>Now let's create vectors for all of our tweets.</p> In\u00a0[20]: Copied! <pre>train_vectors = count_vectorizer.fit_transform(train_df[\"text\"])\ntest_vectors = count_vectorizer.transform(test_df[\"text\"])\n</pre> train_vectors = count_vectorizer.fit_transform(train_df[\"text\"]) test_vectors = count_vectorizer.transform(test_df[\"text\"]) In\u00a0[22]: Copied! <pre>## Our vectors are really big, so we want to push our model's weights toward 0 without completely discounting different words - ridge regression is a good way to do this.\nclf = linear_model.RidgeClassifier()\n</pre> ## Our vectors are really big, so we want to push our model's weights toward 0 without completely discounting different words - ridge regression is a good way to do this. clf = linear_model.RidgeClassifier() <p>Let's test our model and see how well it does on the training data. For this we'll use cross-validation - where we train on a portion of the known data, then validate it with the rest. If we do this several times (with different portions) we can get a good idea for how a particular model or method performs.</p> <p>Here, we are using F1 score as the performance evaluation metric for the model.</p> In\u00a0[23]: Copied! <pre>scores = model_selection.cross_val_score(clf, train_vectors, train_df[\"target\"], cv=3, scoring=\"f1\")\nscores\n</pre> scores = model_selection.cross_val_score(clf, train_vectors, train_df[\"target\"], cv=3, scoring=\"f1\") scores Out[23]: <pre>array([0.59421842, 0.56498283, 0.64082434])</pre> <p>The above scores aren't terrible! It indicates that our hypothesis is approximately 65% likely. Let's continue moving forward, fit the model and predict the test set.</p> In\u00a0[24]: Copied! <pre>clf.fit(train_vectors, train_df[\"target\"])\n</pre> clf.fit(train_vectors, train_df[\"target\"]) Out[24]: <pre>RidgeClassifier()</pre> In\u00a0[26]: Copied! <pre>sample_submission = pd.read_csv(\"https://static-1300131294.cos.ap-shanghai.myqcloud.com/data/deep-learning/nlp/disaster_tweets_test.csv\")\nsample_submission[\"target\"] = clf.predict(test_vectors)\nsample_submission.head()\n</pre> sample_submission = pd.read_csv(\"https://static-1300131294.cos.ap-shanghai.myqcloud.com/data/deep-learning/nlp/disaster_tweets_test.csv\") sample_submission[\"target\"] = clf.predict(test_vectors) sample_submission.head() Out[26]: id keyword location text target 0 0 NaN NaN Just happened a terrible car crash 0 1 2 NaN NaN Heard about #earthquake is different cities, s... 1 2 3 NaN NaN there is a forest fire at spot pond, geese are... 1 3 9 NaN NaN Apocalypse lighting. #Spokane #wildfires 0 4 11 NaN NaN Typhoon Soudelor kills 28 in China and Taiwan 1 <p>The above is the result of the model. Because only the linear regression model is used, the effect is not ideal, but for beginners, it is a good opportunity to understand natural language processing technology.</p> <pre><code>{tableofcontents}\n\n</code></pre>"},{"location":"notebook/nlp/#natural-language-processing","title":"Natural Language Processing\u00b6","text":""},{"location":"notebook/nlp/#what-is-natural-language-processing","title":"What is Natural Language Processing?\u00b6","text":""},{"location":"notebook/nlp/#components-of-nlp","title":"Components of NLP\u00b6","text":""},{"location":"notebook/nlp/#syntax","title":"Syntax\u00b6","text":"<ul> <li>Definition: Syntax pertains to the arrangement of words and phrases to create well-structured sentences in a language.</li> <li>Example: Consider the sentence \"The cat sat on the mat.\" Syntax involves analyzing the grammatical structure of this sentence, ensuring that it adheres to the grammatical rules of English, such as subject-verb agreement and proper word order.</li> </ul>"},{"location":"notebook/nlp/#semantics","title":"Semantics\u00b6","text":"<ul> <li>Definition: Semantics is concerned with understanding the meaning of words and how they create meaning when combined in sentences.</li> <li>Example: In the sentence \"The panda eats shoots and leaves,\" semantics helps distinguish whether the panda eats plants (shoots and leaves) or is involved in a violent act (shoots) and then departs (leaves), based on the meaning of the words and the context.</li> </ul>"},{"location":"notebook/nlp/#pragmatics","title":"Pragmatics\u00b6","text":"<ul> <li>Definition: Pragmatics deals with understanding language in various contexts, ensuring that the intended meaning is derived based on the situation, speaker\u2019s intent, and shared knowledge.</li> <li>Example: If someone says, \"Can you pass the salt?\" Pragmatics involves understanding that this is a request rather than a question about one's ability to pass the salt, interpreting the speaker\u2019s intent based on the dining context.</li> </ul>"},{"location":"notebook/nlp/#discourse","title":"Discourse\u00b6","text":"<ul> <li>Definition: Discourse focuses on the analysis and interpretation of language beyond the sentence level, considering how sentences relate to each other in texts and conversations.</li> <li>Example: In a conversation where one person says, \"I\u2019m freezing,\" and another responds, \"I\u2019ll close the window,\" discourse involves understanding the coherence between the two statements, recognizing that the second statement is a response to the implied request in the first.</li> </ul>"},{"location":"notebook/nlp/#what-is-nlp-used-for","title":"What is NLP Used For?\u00b6","text":""},{"location":"notebook/nlp/#healthcare","title":"Healthcare\u00b6","text":"<p>NLP assists in transcribing and organizing clinical notes, ensuring accurate and efficient documentation of patient information. For instance, a physician might dictate their notes, which NLP systems transcribe into text. Advanced NLP models can further categorize the information, identifying symptoms, diagnoses, and prescribed treatments, thereby streamlining the documentation process, minimizing manual data entry, and enhancing the accuracy of electronic health records.</p>"},{"location":"notebook/nlp/#finance","title":"Finance\u00b6","text":"<p>Financial institutions leverage NLP to perform sentiment analysis on various text data like news articles, financial reports, and social media posts to gauge market sentiment regarding specific stocks or the market in general. Algorithms analyze the frequency of positive or negative words, and through machine learning models, predict potential impacts on stock prices or market movements, aiding traders and investors in making informed decisions.</p>"},{"location":"notebook/nlp/#customer-service","title":"Customer Service\u00b6","text":"<p>NLP-powered chatbots have revolutionized customer support by providing instant, 24/7 responses to customer inquiries. These chatbots understand customer queries through text or voice, interpret the underlying intent, and provide accurate responses or solutions. For instance, a customer might inquire about their order status, and the chatbot, integrating with the order management system, retrieves and delivers the real-time status, enhancing customer experience and reducing support workload.</p>"},{"location":"notebook/nlp/#e-commerce","title":"E-Commerce\u00b6","text":"<p>NLP significantly enhances on-site search functionality in e-commerce platforms by understanding and interpreting user queries, even if they are phrased in a conversational manner or contain typos. For example, if a user searches for \u201cblu jeens,\u201d NLP algorithms correct the typos and understand the intent, providing relevant results for \u201cblue jeans,\u201d thereby ensuring that users find what they are looking for, even with imprecise queries.</p>"},{"location":"notebook/nlp/#legal","title":"Legal\u00b6","text":"<p>In the legal sector, NLP is utilized to automate document review processes, significantly reducing the manual effort involved in sifting through vast volumes of legal documents. For instance, during litigation, legal professionals need to review numerous documents to identify relevant information. NLP algorithms can scan through these documents, identify and highlight pertinent information, such as specific terms, dates, or clauses, thereby expediting the review process and ensuring that no critical information is overlooked.</p>"},{"location":"notebook/nlp/#everyday-applications","title":"Everyday applications\u00b6","text":"<p>Beyond industry-specific applications, NLP is ingrained in our daily lives, making technology more accessible and user-friendly. Here are some everyday applications of NLP:</p> <ul> <li>Search engines. NLP is fundamental to the functioning of search engines, enabling them to understand user queries and provide relevant results.</li> <li>Virtual assistants. Siri, Alexa, and Google Assistant are examples of virtual assistants that use NLP to understand and respond to user commands.</li> <li>Translation services. Services like Google Translate employ NLP to provide real-time language translation, breaking down language barriers and fostering communication.</li> <li>Email filtering. NLP is used in email services to filter out spam and categorize emails, helping users manage their inboxes more effectively.</li> <li>Social media monitoring. NLP enables the analysis of social media content to gauge public opinion, track trends, and manage online reputation.</li> </ul>"},{"location":"notebook/nlp/#overcoming-nlp-challenges","title":"Overcoming NLP challenges\u00b6","text":""},{"location":"notebook/nlp/#code","title":"Code\u00b6","text":""},{"location":"notebook/nlp/#building-vectors","title":"Building vectors\u00b6","text":"<p>The theory behind the model we'll build in this section is pretty simple: the words contained in each tweet are a good indicator of whether they're about a real disaster or not (this is not entirely correct, but it's a great place to start).</p> <p>We'll use scikit-learn's CountVectorizer to count the words in each tweet and turn them into data our machine learning model can process.</p> <p>Note: a vector is, in this context, a set of numbers that a machine learning model can work with. We'll look at one in just a second.</p>"},{"location":"notebook/nlp/#model","title":"Model\u00b6","text":"<p>As we mentioned above, we think the words contained in each tweet are a good indicator of whether they're about a real disaster or not. The presence of particular word (or set of words) in a tweet might link directly to whether or not that tweet is real.</p> <p>What we're assuming here is a linear connection. So let's build a linear model and see!</p>"},{"location":"notebook/nlp/#your-turn","title":"Your turn! \ud83d\ude80\u00b6","text":"<p>You can practice your nlp skills by following the assignment getting start nlp with classification task.</p>"},{"location":"notebook/nlp/#acknowledgments","title":"Acknowledgments\u00b6","text":"<p>Thanks to Matt Crabtree and Phil Culliton for creating the open-source course What is Natural Language Processing (NLP)? and NLP Getting Started Tutorial. It inspires the majority of the content in this chapter.</p>"},{"location":"notebook/text-preprocessing/","title":"Text Preprocessing","text":"In\u00a0[\u00a0]: hide-cell Copied! <pre># Install the necessary dependencies\n\nimport os\nimport sys\n!{sys.executable} -m pip install --quiet pandas scikit-learn numpy matplotlib jupyterlab_myst ipython nltk\n</pre> # Install the necessary dependencies  import os import sys !{sys.executable} -m pip install --quiet pandas scikit-learn numpy matplotlib jupyterlab_myst ipython nltk <p>license: code: MIT content: CC-BY-4.0 github: https://github.com/ocademy-ai/machine-learning venue: By Ocademy open_access: true bibliography:</p> <ul> <li>https://raw.githubusercontent.com/ocademy-ai/machine-learning/main/open-machine-learning-jupyter-book/references.bib</li> </ul> In\u00a0[2]: Copied! <pre>import nltk\n\n# input text\ntext = \"Natural language processing is a field of artificial intelligence that deals with the interaction between computers and human (natural) language.\"\n\n# tokenize the text\ntokens = nltk.word_tokenize(text)\n\nprint(\"Tokens:\", tokens)\n</pre> import nltk  # input text text = \"Natural language processing is a field of artificial intelligence that deals with the interaction between computers and human (natural) language.\"  # tokenize the text tokens = nltk.word_tokenize(text)  print(\"Tokens:\", tokens) <pre>Tokens: ['Natural', 'language', 'processing', 'is', 'a', 'field', 'of', 'artificial', 'intelligence', 'that', 'deals', 'with', 'the', 'interaction', 'between', 'computers', 'and', 'human', '(', 'natural', ')', 'language', '.']\n</pre> In\u00a0[3]: Copied! <pre>import nltk\n\n# input text\ntext = \"Natural language processing is a field of artificial intelligence that deals with the interaction between computers and human (natural) language.\"\n\n# tokenize the text\ntokens = nltk.word_tokenize(text)\n\n# get list of stopwords in English\nstopwords = nltk.corpus.stopwords.words(\"english\")\n\n# remove stopwords\nfiltered_tokens = [token for token in tokens if token.lower() not in stopwords]\n\nprint(\"Tokens without stopwords:\", filtered_tokens)\n</pre> import nltk  # input text text = \"Natural language processing is a field of artificial intelligence that deals with the interaction between computers and human (natural) language.\"  # tokenize the text tokens = nltk.word_tokenize(text)  # get list of stopwords in English stopwords = nltk.corpus.stopwords.words(\"english\")  # remove stopwords filtered_tokens = [token for token in tokens if token.lower() not in stopwords]  print(\"Tokens without stopwords:\", filtered_tokens) <pre>Tokens without stopwords: ['Natural', 'language', 'processing', 'field', 'artificial', 'intelligence', 'deals', 'interaction', 'computers', 'human', '(', 'natural', ')', 'language', '.']\n</pre> Image: Stemming in NLP In\u00a0[4]: Copied! <pre>import nltk\n\n# input text\ntext = \"Natural language processing is a field of artificial intelligence that deals with the interaction between computers and human (natural) language.\"\n\n# tokenize the text\ntokens = nltk.word_tokenize(text)\n\n# create stemmer object\nstemmer = nltk.stem.PorterStemmer()\n\n# stem each token\nstemmed_tokens = [stemmer.stem(token) for token in tokens]\n\nprint(\"Stemmed tokens:\", stemmed_tokens)\n</pre> import nltk  # input text text = \"Natural language processing is a field of artificial intelligence that deals with the interaction between computers and human (natural) language.\"  # tokenize the text tokens = nltk.word_tokenize(text)  # create stemmer object stemmer = nltk.stem.PorterStemmer()  # stem each token stemmed_tokens = [stemmer.stem(token) for token in tokens]  print(\"Stemmed tokens:\", stemmed_tokens) <pre>Stemmed tokens: ['natur', 'languag', 'process', 'is', 'a', 'field', 'of', 'artifici', 'intellig', 'that', 'deal', 'with', 'the', 'interact', 'between', 'comput', 'and', 'human', '(', 'natur', ')', 'languag', '.']\n</pre> Image: Lemmatization in NLP <p>To perform lemmatization on a list of tokens using NLTK, you can use the nltk.stem.WordNetLemmatizer() function to create a lemmatizer object and the lemmatize() method to lemmatize each token. Here is an example of how to do this:</p> In\u00a0[5]: Copied! <pre>import nltk\n\n# input text\ntext = \"Natural language processing is a field of artificial intelligence that deals with the interaction between computers and human (natural) language.\"\n\n# tokenize the text\ntokens = nltk.word_tokenize(text)\n\n# create lemmatizer object\nlemmatizer = nltk.stem.WordNetLemmatizer()\n\n# lemmatize each token\nlemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n\nprint(\"Lemmatized tokens:\", lemmatized_tokens)\n</pre> import nltk  # input text text = \"Natural language processing is a field of artificial intelligence that deals with the interaction between computers and human (natural) language.\"  # tokenize the text tokens = nltk.word_tokenize(text)  # create lemmatizer object lemmatizer = nltk.stem.WordNetLemmatizer()  # lemmatize each token lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]  print(\"Lemmatized tokens:\", lemmatized_tokens) <pre>Lemmatized tokens: ['Natural', 'language', 'processing', 'is', 'a', 'field', 'of', 'artificial', 'intelligence', 'that', 'deal', 'with', 'the', 'interaction', 'between', 'computer', 'and', 'human', '(', 'natural', ')', 'language', '.']\n</pre> Image: POS tag and description <p>To perform part of speech (POS) tagging on a list of tokens using NLTK, you can use the nltk.pos_tag() function to tag the tokens with their corresponding POS tags. Here is an example of how to do this:</p> In\u00a0[1]: Copied! <pre>import nltk\n\n# input text\ntext = \"Natural language processing is a field of artificial intelligence that deals with the interaction between computers and human (natural) language.\"\n\n# tokenize the text\ntokens = nltk.word_tokenize(text)\n\n# tag the tokens with their POS tags\ntagged_tokens = nltk.pos_tag(tokens)\n\nprint(\"Tagged tokens:\", tagged_tokens)\n</pre> import nltk  # input text text = \"Natural language processing is a field of artificial intelligence that deals with the interaction between computers and human (natural) language.\"  # tokenize the text tokens = nltk.word_tokenize(text)  # tag the tokens with their POS tags tagged_tokens = nltk.pos_tag(tokens)  print(\"Tagged tokens:\", tagged_tokens) <pre>Tagged tokens: [('Natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('field', 'NN'), ('of', 'IN'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('that', 'IN'), ('deals', 'NNS'), ('with', 'IN'), ('the', 'DT'), ('interaction', 'NN'), ('between', 'IN'), ('computers', 'NNS'), ('and', 'CC'), ('human', 'JJ'), ('(', '('), ('natural', 'JJ'), (')', ')'), ('language', 'NN'), ('.', '.')]\n</pre> Image: NER in NLP <p>To perform named entity recognition (NER) on a list of tokens using NLTK, you can use the nltk.ne_chunk() function to identify and label named entities in the tokens. Here is an example of how to do this:</p> In\u00a0[7]: Copied! <pre>import nltk\n\n# input text\ntext = \"Natural language processing is a field of artificial intelligence that deals with the interaction between computers and human (natural) language. John Smith works at Google in New York.\"\n\n# tokenize the text\ntokens = nltk.word_tokenize(text)\n\n# tag the tokens with their part of speech\ntagged_tokens = nltk.pos_tag(tokens)\n\n# identify named entities\nnamed_entities = nltk.ne_chunk(tagged_tokens)\n\nprint(\"Named entities:\", named_entities)\n</pre> import nltk  # input text text = \"Natural language processing is a field of artificial intelligence that deals with the interaction between computers and human (natural) language. John Smith works at Google in New York.\"  # tokenize the text tokens = nltk.word_tokenize(text)  # tag the tokens with their part of speech tagged_tokens = nltk.pos_tag(tokens)  # identify named entities named_entities = nltk.ne_chunk(tagged_tokens)  print(\"Named entities:\", named_entities) <pre>Named entities: (S\n  Natural/JJ\n  language/NN\n  processing/NN\n  is/VBZ\n  a/DT\n  field/NN\n  of/IN\n  artificial/JJ\n  intelligence/NN\n  that/IN\n  deals/NNS\n  with/IN\n  the/DT\n  interaction/NN\n  between/IN\n  computers/NNS\n  and/CC\n  human/JJ\n  (/(\n  natural/JJ\n  )/)\n  language/NN\n  ./.\n  (PERSON John/NNP Smith/NNP)\n  works/VBZ\n  at/IN\n  (ORGANIZATION Google/NNP)\n  in/IN\n  (GPE New/NNP York/NNP)\n  ./.)\n</pre> <p>Preprocessing techniques can be applied independently or in combination, depending on the specific requirements of the task at hand.</p> <p>Here is an example of a typical NLP pipeline using the NLTK:</p> <p>1.Tokenization: First, we need to split the input text into individual words (tokens). This can be done using the nltk.word_tokenize() function.</p> <p>2.Part-of-speech tagging: Next, we can use the nltk.pos_tag() function to assign a part-of-speech (POS) tag to each token, which indicates its role in a sentence (e.g., noun, verb, adjective).</p> <p>3.Named entity recognition: Using the nltk.ne_chunk() function, we can identify named entities (e.g., person, organization, location) in the text.</p> <p>4.Lemmatization: We can use the nltk.WordNetLemmatizer() function to convert each token to its base form (lemma), which helps with the analysis of the text.</p> <p>5.Stopword removal: We can use the nltk.corpus.stopwords.words() function to remove common words (stopwords) that do not add significant meaning to the text, such as \u201cthe,\u201d \u201ca,\u201d and \u201can.\u201d</p> <p>6.Text classification: Finally, we can use the processed text to train a classifier using machine learning algorithms to perform tasks such as sentiment analysis or spam detection.</p> <p>First, we preprocess the text, including tokenization, part-of-speech tagging, named entity recognition, lemmatization and stopword removal.</p> In\u00a0[2]: Copied! <pre>import nltk\n\n# input text\ntext = \"Natural language processing is a field of artificial intelligence that deals with the interaction between computers and human (natural) language.\"\n\n# tokenization\ntokens = nltk.word_tokenize(text)\n\n# part-of-speech tagging\npos_tags = nltk.pos_tag(tokens)\n\n# named entity recognition\nnamed_entities = nltk.ne_chunk(pos_tags)\n\n# lemmatization\nlemmatizer = nltk.WordNetLemmatizer()\nlemmas = [lemmatizer.lemmatize(token) for token in tokens]\n\n# stopword removal\nstopwords = nltk.corpus.stopwords.words(\"english\")\nfiltered_tokens = [token for token in tokens if token not in stopwords]\n</pre> import nltk  # input text text = \"Natural language processing is a field of artificial intelligence that deals with the interaction between computers and human (natural) language.\"  # tokenization tokens = nltk.word_tokenize(text)  # part-of-speech tagging pos_tags = nltk.pos_tag(tokens)  # named entity recognition named_entities = nltk.ne_chunk(pos_tags)  # lemmatization lemmatizer = nltk.WordNetLemmatizer() lemmas = [lemmatizer.lemmatize(token) for token in tokens]  # stopword removal stopwords = nltk.corpus.stopwords.words(\"english\") filtered_tokens = [token for token in tokens if token not in stopwords] <p>Then we do the text classification (example using a simple Naive Bayes classifier).</p> In\u00a0[15]: Copied! <pre>from nltk.classify import NaiveBayesClassifier\n\n# training data (using a toy dataset for illustration purposes)\ntraining_data = [(\"I enjoy the book.\", \"pos\"),(\"I like this movie.\", \"pos\"),(\"It was a boring movie.\", \"neg\")]\n\n# extract features from the training data\ndef extract_features(text):\n    features = {}\n    for word in nltk.word_tokenize(text):\n        features[word] = True\n    return features\n\n# create a list of feature sets and labels\nfeature_sets = [(extract_features(text), label) for (text, label) in training_data]\n# train the classifier\nclassifier = NaiveBayesClassifier.train(feature_sets)\n\n# test the classifier on a new example\ntest_text = \"I enjoyed the movie.\"\nprint(\"Sentiment:\", classifier.classify(extract_features(test_text)))\n</pre> from nltk.classify import NaiveBayesClassifier  # training data (using a toy dataset for illustration purposes) training_data = [(\"I enjoy the book.\", \"pos\"),(\"I like this movie.\", \"pos\"),(\"It was a boring movie.\", \"neg\")]  # extract features from the training data def extract_features(text):     features = {}     for word in nltk.word_tokenize(text):         features[word] = True     return features  # create a list of feature sets and labels feature_sets = [(extract_features(text), label) for (text, label) in training_data] # train the classifier classifier = NaiveBayesClassifier.train(feature_sets)  # test the classifier on a new example test_text = \"I enjoyed the movie.\" print(\"Sentiment:\", classifier.classify(extract_features(test_text))) <pre>Sentiment: pos\n</pre>"},{"location":"notebook/text-preprocessing/#text-preprocessing","title":"Text Preprocessing\u00b6","text":"<p>Preprocessing in NLP is a means to get text data ready for further processing or analysis. Most of the time, preprocessing is a mix of cleaning and normalising techniques that make the text easier to use for the task at hand.</p> <p>A useful library for processing text in Python is the Natural Language Toolkit (NLTK). This chapter will go into 6 of the most commonly used pre-processing steps and provide code examples so you can start using the techniques immediately.</p>"},{"location":"notebook/text-preprocessing/#common-nltk-preprocessing-steps","title":"Common NLTK preprocessing steps\u00b6","text":""},{"location":"notebook/text-preprocessing/#tokenization","title":"Tokenization\u00b6","text":"<p>Splitting the text into individual words or subwords (tokens).</p>"},{"location":"notebook/text-preprocessing/#figure-httpsstatic-1300131294cosap-shanghaimyqcloudcomimagesdeep-learningnlptokenizationpng","title":":::{figure} https://static-1300131294.cos.ap-shanghai.myqcloud.com/images/deep-learning/NLP/tokenization.png\u00b6","text":""},{"location":"notebook/text-preprocessing/#name-tokenization-in-nlp-width-90","title":"name: 'tokenization in nlp' width: 90%\u00b6","text":"<p>Tokenization in NLP :::</p> <p>Here is how to implement tokenization in NLTK:</p>"},{"location":"notebook/text-preprocessing/#remove-stop-words","title":"Remove stop words\u00b6","text":"<p>Removing common words that do not add significant meaning to the text, such as \u201ca,\u201d \u201can,\u201d and \u201cthe.\u201d</p> <p>To remove common stop words from a list of tokens using NLTK, you can use the nltk.corpus.stopwords.words() function to get a list of stopwords in a specific language and filter the tokens using this list. Here is an example of how to do this:</p>"},{"location":"notebook/text-preprocessing/#stemming","title":"Stemming\u00b6","text":"<p>Reducing words to their root form by removing suffixes and prefixes, such as converting \u201cjumping\u201d to \u201cjump.\u201d  But it may produce non-existent words.</p>"},{"location":"notebook/text-preprocessing/#lemmatization","title":"Lemmatization\u00b6","text":"<p>Reducing words to their base form by considering the context in which they are used, such as \u201crunning\u201d or \u201cran\u201d becoming \u201crun\u201d. This technique is similar to stemming, but it is more accurate as it considers the context of the word.</p>"},{"location":"notebook/text-preprocessing/#part-of-speech-tagging","title":"Part Of Speech Tagging\u00b6","text":"<p>Identifying the part of speech of each word in the text, such as noun, verb, or adjective.</p>"},{"location":"notebook/text-preprocessing/#named-entity-recognitionner","title":"Named Entity Recognition(NER)\u00b6","text":"<p>Extracting named entities from a text, like a person\u2019s name.</p>"},{"location":"notebook/text-preprocessing/#nltk-preprocessing-pipeline-example","title":"NLTK preprocessing pipeline example\u00b6","text":""},{"location":"notebook/text-preprocessing/#nltk-preprocessing-example-code","title":"NLTK preprocessing example code\u00b6","text":""},{"location":"notebook/text-preprocessing/#your-turn","title":"Your turn! \ud83d\ude80\u00b6","text":"<p>Assignment - Beginner Guide to Text Pre-Processing</p>"},{"location":"notebook/text-preprocessing/#acknowledgments","title":"Acknowledgments\u00b6","text":"<p>Thanks to Neri Van Otten for creating the open-source project Top 14 Steps To Build A Complete NLTK Preprocessing Pipeline In Python.It inspire the majority of the content in this chapter.</p>"},{"location":"notebook/text-representation/","title":"Word embedding","text":"In\u00a0[\u00a0]: hide-cell Copied! <pre># Install the necessary dependencies\n\nimport os\nimport sys\n!{sys.executable} -m pip install --quiet pandas scikit-learn numpy matplotlib jupyterlab_myst ipython gensim torch\n</pre> # Install the necessary dependencies  import os import sys !{sys.executable} -m pip install --quiet pandas scikit-learn numpy matplotlib jupyterlab_myst ipython gensim torch <p>license: code: MIT content: CC-BY-4.0 github: https://github.com/ocademy-ai/machine-learning venue: By Ocademy open_access: true bibliography:</p> <ul> <li>https://raw.githubusercontent.com/ocademy-ai/machine-learning/main/open-machine-learning-jupyter-book/references.bib</li> </ul> <p>Word Embeddings are numeric representations of words in a lower-dimensional space, capturing semantic and syntactic information. Mainly including discrete representation methods and distribution representation methods</p> <p>This method involves compiling a list of distinct terms and giving each one a unique integer value, or id. and after that, insert each word\u2019s distinct id into the sentence. Every vocabulary word is handled as a feature in this instance. Thus, a large vocabulary will result in an extremely large feature size. Common discrete representation methods include:one-hot,Bag of Word (Bow) and Term frequency-inverse document frequency (TF-IDF)</p> <p>One-hot encoding is a simple method for representing words in natural language processing (NLP). In this encoding scheme, each word in the vocabulary is represented as a unique vector, where the dimensionality of the vector is equal to the size of the vocabulary. The vector has all elements set to 0, except for the element corresponding to the index of the word in the vocabulary, which is set to 1.</p> In\u00a0[5]: Copied! <pre>def one_hot_encode(text):\n\twords = text.split()\n\tvocabulary = set(words)\n\tword_to_index = {word: i for i, word in enumerate(vocabulary)}\n\tone_hot_encoded = []\n\tfor word in words:\n\t\tone_hot_vector = [0] * len(vocabulary)\n\t\tone_hot_vector[word_to_index[word]] = 1\n\t\tone_hot_encoded.append(one_hot_vector)\n\n\treturn one_hot_encoded, word_to_index, vocabulary\n\n# sample\nexample_text = \"cat in the hat dog on the mat bird in the tree\"\n\none_hot_encoded, word_to_index, vocabulary = one_hot_encode(example_text)\n\nprint(\"Vocabulary:\", vocabulary)\nprint(\"Word to Index Mapping:\", word_to_index)\nprint(\"One-Hot Encoded Matrix:\")\nfor word, encoding in zip(example_text.split(), one_hot_encoded):\n\tprint(f\"{word}: {encoding}\")\n</pre> def one_hot_encode(text): \twords = text.split() \tvocabulary = set(words) \tword_to_index = {word: i for i, word in enumerate(vocabulary)} \tone_hot_encoded = [] \tfor word in words: \t\tone_hot_vector = [0] * len(vocabulary) \t\tone_hot_vector[word_to_index[word]] = 1 \t\tone_hot_encoded.append(one_hot_vector)  \treturn one_hot_encoded, word_to_index, vocabulary  # sample example_text = \"cat in the hat dog on the mat bird in the tree\"  one_hot_encoded, word_to_index, vocabulary = one_hot_encode(example_text)  print(\"Vocabulary:\", vocabulary) print(\"Word to Index Mapping:\", word_to_index) print(\"One-Hot Encoded Matrix:\") for word, encoding in zip(example_text.split(), one_hot_encoded): \tprint(f\"{word}: {encoding}\")  <pre>Vocabulary: {'mat', 'dog', 'tree', 'in', 'on', 'hat', 'the', 'bird', 'cat'}\nWord to Index Mapping: {'mat': 0, 'dog': 1, 'tree': 2, 'in': 3, 'on': 4, 'hat': 5, 'the': 6, 'bird': 7, 'cat': 8}\nOne-Hot Encoded Matrix:\ncat: [0, 0, 0, 0, 0, 0, 0, 0, 1]\nin: [0, 0, 0, 1, 0, 0, 0, 0, 0]\nthe: [0, 0, 0, 0, 0, 0, 1, 0, 0]\nhat: [0, 0, 0, 0, 0, 1, 0, 0, 0]\ndog: [0, 1, 0, 0, 0, 0, 0, 0, 0]\non: [0, 0, 0, 0, 1, 0, 0, 0, 0]\nthe: [0, 0, 0, 0, 0, 0, 1, 0, 0]\nmat: [1, 0, 0, 0, 0, 0, 0, 0, 0]\nbird: [0, 0, 0, 0, 0, 0, 0, 1, 0]\nin: [0, 0, 0, 1, 0, 0, 0, 0, 0]\nthe: [0, 0, 0, 0, 0, 0, 1, 0, 0]\ntree: [0, 0, 1, 0, 0, 0, 0, 0, 0]\n</pre> <p>Bag-of-Words (BoW) is a text representation technique that represents a document as an unordered set of words and their respective frequencies. It discards the word order and captures the frequency of each word in the document, creating a vector representation.</p> In\u00a0[3]: Copied! <pre>from sklearn.feature_extraction.text import CountVectorizer\ndocuments = [\"This is the first document.\",\n\t\t\t\"This document is the second document.\",\n\t\t\t\"And this is the third one.\",\n\t\t\t\"Is this the first document?\"]\n\nvectorizer = CountVectorizer()\nX = vectorizer.fit_transform(documents)\nfeature_names = vectorizer.get_feature_names_out()\n\nprint(\"Bag-of-Words Matrix:\")\nprint(X.toarray())\nprint(\"Vocabulary (Feature Names):\", feature_names)\n</pre> from sklearn.feature_extraction.text import CountVectorizer documents = [\"This is the first document.\", \t\t\t\"This document is the second document.\", \t\t\t\"And this is the third one.\", \t\t\t\"Is this the first document?\"]  vectorizer = CountVectorizer() X = vectorizer.fit_transform(documents) feature_names = vectorizer.get_feature_names_out()  print(\"Bag-of-Words Matrix:\") print(X.toarray()) print(\"Vocabulary (Feature Names):\", feature_names)  <pre>Bag-of-Words Matrix:\n[[0 1 1 1 0 0 1 0 1]\n [0 2 0 1 0 1 1 0 1]\n [1 0 0 1 1 0 1 1 1]\n [0 1 1 1 0 0 1 0 1]]\nVocabulary (Feature Names): ['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n</pre> <p>Term Frequency-Inverse Document Frequency, commonly known as TF-IDF, is a numerical statistic that reflects the importance of a word in a document relative to a collection of documents (corpus). It is widely used in natural language processing and information retrieval to evaluate the significance of a term within a specific document in a larger corpus. TF-IDF consists of two components:</p> <p>\u00b7Term Frequency (TF): Term Frequency measures how often a term (word) appears in a document. It is calculated using the formula:</p> <p>$TF(t,d)=\\frac{Total\\ number\\ of\\ times\\ term\\ t\\ appears\\ in\\ document\\ d}{Total\\ number\\ of\\ terms\\ in\\ document\\ d}$</p> <p>\u00b7Inverse Document Frequency (IDF): Inverse Document Frequency measures the importance of a term across a collection of documents. It is calculated using the formula:</p> <p>$IDF(t,D)=\\log{(\\frac{Total\\ documents}{Number\\ of\\ documents\\ containing\\ term\\ t})}$</p> <p>The TF-IDF score for a term t in a document d is then given by multiplying the TF and IDF values:</p> <p>TF-IDF(t,d,D)=TF(t,d)\u00d7IDF(t,D)</p> <p>The higher the TF-IDF score for a term in a document, the more important that term is to that document within the context of the entire corpus. This weighting scheme helps in identifying and extracting relevant information from a large collection of documents, and it is commonly used in text mining, information retrieval, and document clustering.</p> <p>Let\u2019s Implement Term Frequency-Inverse Document Frequency (TF-IDF) using python with the scikit-learn library. It begins by defining a set of sample documents. The TfidfVectorizer is employed to transform these documents into a TF-IDF matrix. The code then extracts and prints the TF-IDF values for each word in each document. This statistical measure helps assess the importance of words in a document relative to their frequency across a collection of documents, aiding in information retrieval and text analysis tasks.</p> In\u00a0[4]: Copied! <pre>from sklearn.feature_extraction.text import TfidfVectorizer\n\n# Sample\ndocuments = [\n\t\"The quick brown fox jumps over the lazy dog.\",\n\t\"A journey of a thousand miles begins with a single step.\",\n]\n\nvectorizer = TfidfVectorizer() # Create the TF-IDF vectorizer\ntfidf_matrix = vectorizer.fit_transform(documents)\nfeature_names = vectorizer.get_feature_names_out()\ntfidf_values = {}\n\nfor doc_index, doc in enumerate(documents):\n\tfeature_index = tfidf_matrix[doc_index, :].nonzero()[1]\n\ttfidf_doc_values = zip(feature_index, [tfidf_matrix[doc_index, x] for x in feature_index])\n\ttfidf_values[doc_index] = {feature_names[i]: value for i, value in tfidf_doc_values}\n#let's print\nfor doc_index, values in tfidf_values.items():\n\tprint(f\"Document {doc_index + 1}:\")\n\tfor word, tfidf_value in values.items():\n\t\tprint(f\"{word}: {tfidf_value}\")\n\tprint(\"\\n\")\n</pre> from sklearn.feature_extraction.text import TfidfVectorizer  # Sample documents = [ \t\"The quick brown fox jumps over the lazy dog.\", \t\"A journey of a thousand miles begins with a single step.\", ]  vectorizer = TfidfVectorizer() # Create the TF-IDF vectorizer tfidf_matrix = vectorizer.fit_transform(documents) feature_names = vectorizer.get_feature_names_out() tfidf_values = {}  for doc_index, doc in enumerate(documents): \tfeature_index = tfidf_matrix[doc_index, :].nonzero()[1] \ttfidf_doc_values = zip(feature_index, [tfidf_matrix[doc_index, x] for x in feature_index]) \ttfidf_values[doc_index] = {feature_names[i]: value for i, value in tfidf_doc_values} #let's print for doc_index, values in tfidf_values.items(): \tprint(f\"Document {doc_index + 1}:\") \tfor word, tfidf_value in values.items(): \t\tprint(f\"{word}: {tfidf_value}\") \tprint(\"\\n\")  <pre>Document 1:\ndog: 0.30151134457776363\nlazy: 0.30151134457776363\nover: 0.30151134457776363\njumps: 0.30151134457776363\nfox: 0.30151134457776363\nbrown: 0.30151134457776363\nquick: 0.30151134457776363\nthe: 0.6030226891555273\n\n\nDocument 2:\nstep: 0.3535533905932738\nsingle: 0.3535533905932738\nwith: 0.3535533905932738\nbegins: 0.3535533905932738\nmiles: 0.3535533905932738\nthousand: 0.3535533905932738\nof: 0.3535533905932738\njourney: 0.3535533905932738\n\n\n</pre> <p>Word2Vec is a neural approach for generating word embeddings. It belongs to the family of neural word embedding techniques and specifically falls under the category of distributed representation models. It is a popular technique in natural language processing (NLP) that is used to represent words as continuous vector spaces. Developed by a team at Google, Word2Vec aims to capture the semantic relationships between words by mapping them to high-dimensional vectors. The underlying idea is that words with similar meanings should have similar vector representations. In Word2Vec every word is assigned a vector. We start with either a random vector or one-hot vector.</p> <p>There are two neural embedding methods for Word2Vec, Continuous Bag of Words (CBOW) and Skip-gram.</p> <p>Continuous Bag of Words (CBOW) is a type of neural network architecture used in the Word2Vec model. The primary objective of CBOW is to predict a target word based on its context, which consists of the surrounding words in a given window. Given a sequence of words in a context window, the model is trained to predict the target word at the center of the window.</p> <p>CBOW is a feedforward neural network with a single hidden layer. The input layer represents the context words, and the output layer represents the target word. The hidden layer contains the learned continuous vector representations (word embeddings) of the input words.</p> <p>The architecture is useful for learning distributed representations of words in a continuous vector space.</p> Image: Continuous Bag of Words <p>The hidden layer contains the continuous vector representations (word embeddings) of the input words.</p> <p>The weights between the input layer and the hidden layer are learned during training. The dimensionality of the hidden layer represents the size of the word embeddings (the continuous vector space).</p> In\u00a0[6]: Copied! <pre>import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Define CBOW model\nclass CBOWModel(nn.Module):\n\tdef __init__(self, vocab_size, embed_size):\n\t\tsuper(CBOWModel, self).__init__()\n\t\tself.embeddings = nn.Embedding(vocab_size, embed_size)\n\t\tself.linear = nn.Linear(embed_size, vocab_size)\n\n\tdef forward(self, context):\n\t\tcontext_embeds = self.embeddings(context).sum(dim=1)\n\t\toutput = self.linear(context_embeds)\n\t\treturn output\n\n# Sample data\ncontext_size = 2\nraw_text = \"word embeddings are awesome\"\ntokens = raw_text.split()\nvocab = set(tokens)\nword_to_index = {word: i for i, word in enumerate(vocab)}\ndata = []\nfor i in range(2, len(tokens) - 2):\n\tcontext = [word_to_index[word] for word in tokens[i - 2:i] + tokens[i + 1:i + 3]]\n\ttarget = word_to_index[tokens[i]]\n\tdata.append((torch.tensor(context), torch.tensor(target)))\n\n# Hyperparameters\nvocab_size = len(vocab)\nembed_size = 10\nlearning_rate = 0.01\nepochs = 100\n\n# Initialize CBOW model\ncbow_model = CBOWModel(vocab_size, embed_size)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(cbow_model.parameters(), lr=learning_rate)\n\n# Training loop\nfor epoch in range(epochs):\n\ttotal_loss = 0\n\tfor context, target in data:\n\t\toptimizer.zero_grad()\n\t\toutput = cbow_model(context)\n\t\tloss = criterion(output.unsqueeze(0), target.unsqueeze(0))\n\t\tloss.backward()\n\t\toptimizer.step()\n\t\ttotal_loss += loss.item()\n\tprint(f\"Epoch {epoch + 1}, Loss: {total_loss}\")\n\n# Example usage: Get embedding for a specific word\nword_to_lookup = \"embeddings\"\nword_index = word_to_index[word_to_lookup]\nembedding = cbow_model.embeddings(torch.tensor([word_index]))\nprint(f\"Embedding for '{word_to_lookup}': {embedding.detach().numpy()}\")\n</pre> import torch import torch.nn as nn import torch.optim as optim  # Define CBOW model class CBOWModel(nn.Module): \tdef __init__(self, vocab_size, embed_size): \t\tsuper(CBOWModel, self).__init__() \t\tself.embeddings = nn.Embedding(vocab_size, embed_size) \t\tself.linear = nn.Linear(embed_size, vocab_size)  \tdef forward(self, context): \t\tcontext_embeds = self.embeddings(context).sum(dim=1) \t\toutput = self.linear(context_embeds) \t\treturn output  # Sample data context_size = 2 raw_text = \"word embeddings are awesome\" tokens = raw_text.split() vocab = set(tokens) word_to_index = {word: i for i, word in enumerate(vocab)} data = [] for i in range(2, len(tokens) - 2): \tcontext = [word_to_index[word] for word in tokens[i - 2:i] + tokens[i + 1:i + 3]] \ttarget = word_to_index[tokens[i]] \tdata.append((torch.tensor(context), torch.tensor(target)))  # Hyperparameters vocab_size = len(vocab) embed_size = 10 learning_rate = 0.01 epochs = 100  # Initialize CBOW model cbow_model = CBOWModel(vocab_size, embed_size) criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(cbow_model.parameters(), lr=learning_rate)  # Training loop for epoch in range(epochs): \ttotal_loss = 0 \tfor context, target in data: \t\toptimizer.zero_grad() \t\toutput = cbow_model(context) \t\tloss = criterion(output.unsqueeze(0), target.unsqueeze(0)) \t\tloss.backward() \t\toptimizer.step() \t\ttotal_loss += loss.item() \tprint(f\"Epoch {epoch + 1}, Loss: {total_loss}\")  # Example usage: Get embedding for a specific word word_to_lookup = \"embeddings\" word_index = word_to_index[word_to_lookup] embedding = cbow_model.embeddings(torch.tensor([word_index])) print(f\"Embedding for '{word_to_lookup}': {embedding.detach().numpy()}\")  <pre>Epoch 1, Loss: 0\nEpoch 2, Loss: 0\nEpoch 3, Loss: 0\nEpoch 4, Loss: 0\nEpoch 5, Loss: 0\nEpoch 6, Loss: 0\nEpoch 7, Loss: 0\nEpoch 8, Loss: 0\nEpoch 9, Loss: 0\nEpoch 10, Loss: 0\nEpoch 11, Loss: 0\nEpoch 12, Loss: 0\nEpoch 13, Loss: 0\nEpoch 14, Loss: 0\nEpoch 15, Loss: 0\nEpoch 16, Loss: 0\nEpoch 17, Loss: 0\nEpoch 18, Loss: 0\nEpoch 19, Loss: 0\nEpoch 20, Loss: 0\nEpoch 21, Loss: 0\nEpoch 22, Loss: 0\nEpoch 23, Loss: 0\nEpoch 24, Loss: 0\nEpoch 25, Loss: 0\nEpoch 26, Loss: 0\nEpoch 27, Loss: 0\nEpoch 28, Loss: 0\nEpoch 29, Loss: 0\nEpoch 30, Loss: 0\nEpoch 31, Loss: 0\nEpoch 32, Loss: 0\nEpoch 33, Loss: 0\nEpoch 34, Loss: 0\nEpoch 35, Loss: 0\nEpoch 36, Loss: 0\nEpoch 37, Loss: 0\nEpoch 38, Loss: 0\nEpoch 39, Loss: 0\nEpoch 40, Loss: 0\nEpoch 41, Loss: 0\nEpoch 42, Loss: 0\nEpoch 43, Loss: 0\nEpoch 44, Loss: 0\nEpoch 45, Loss: 0\nEpoch 46, Loss: 0\nEpoch 47, Loss: 0\nEpoch 48, Loss: 0\nEpoch 49, Loss: 0\nEpoch 50, Loss: 0\nEpoch 51, Loss: 0\nEpoch 52, Loss: 0\nEpoch 53, Loss: 0\nEpoch 54, Loss: 0\nEpoch 55, Loss: 0\nEpoch 56, Loss: 0\nEpoch 57, Loss: 0\nEpoch 58, Loss: 0\nEpoch 59, Loss: 0\nEpoch 60, Loss: 0\nEpoch 61, Loss: 0\nEpoch 62, Loss: 0\nEpoch 63, Loss: 0\nEpoch 64, Loss: 0\nEpoch 65, Loss: 0\nEpoch 66, Loss: 0\nEpoch 67, Loss: 0\nEpoch 68, Loss: 0\nEpoch 69, Loss: 0\nEpoch 70, Loss: 0\nEpoch 71, Loss: 0\nEpoch 72, Loss: 0\nEpoch 73, Loss: 0\nEpoch 74, Loss: 0\nEpoch 75, Loss: 0\nEpoch 76, Loss: 0\nEpoch 77, Loss: 0\nEpoch 78, Loss: 0\nEpoch 79, Loss: 0\nEpoch 80, Loss: 0\nEpoch 81, Loss: 0\nEpoch 82, Loss: 0\nEpoch 83, Loss: 0\nEpoch 84, Loss: 0\nEpoch 85, Loss: 0\nEpoch 86, Loss: 0\nEpoch 87, Loss: 0\nEpoch 88, Loss: 0\nEpoch 89, Loss: 0\nEpoch 90, Loss: 0\nEpoch 91, Loss: 0\nEpoch 92, Loss: 0\nEpoch 93, Loss: 0\nEpoch 94, Loss: 0\nEpoch 95, Loss: 0\nEpoch 96, Loss: 0\nEpoch 97, Loss: 0\nEpoch 98, Loss: 0\nEpoch 99, Loss: 0\nEpoch 100, Loss: 0\nEmbedding for 'embeddings': [[-1.0344244   0.07897425  0.03608504 -0.556515    0.14206451 -1.833232\n   0.7083351   0.66258     0.57552516  1.7009653 ]]\n</pre> <p>The Skip-Gram model learns distributed representations of words in a continuous vector space. The main objective of Skip-Gram is to predict context words (words surrounding a target word) given a target word. This is the opposite of the Continuous Bag of Words (CBOW) model, where the objective is to predict the target word based on its context. It is shown that this method produces more meaningful embeddings.</p> Image: NER in NLP <p>After applying the above neural embedding methods we get trained vectors of each word after many iterations through the corpus. These trained vectors preserve syntactical or semantic information and are converted to lower dimensions. The vectors with similar meaning or semantic information are placed close to each other in space.</p> <p>Let\u2019s understand with a basic example. The python code contains, parameter that controls the dimensionality of the word vectors, and you can adjust other parameters such as based on your specific needs.vector_sizewindow</p> <p>Note: Word2Vec models can perform better with larger datasets. If you have a large corpus, you might achieve more meaningful word embeddings.</p> In\u00a0[8]: Copied! <pre>!pip install gensim\nfrom gensim.models import Word2Vec\nfrom nltk.tokenize import word_tokenize\nimport nltk\nnltk.download('punkt') # Download the tokenizer models if not already downloaded\n\nsample = \"Word embeddings are dense vector representations of words.\"\ntokenized_corpus = word_tokenize(sample.lower()) # Lowercasing for consistency\n\nskipgram_model = Word2Vec(sentences=[tokenized_corpus],\n\t\t\t\t\t\tvector_size=100, # Dimensionality of the word vectors\n\t\t\t\t\t\twindow=5,\t\t # Maximum distance between the current and predicted word within a sentence\n\t\t\t\t\t\tsg=1,\t\t\t # Skip-Gram model (1 for Skip-Gram, 0 for CBOW)\n\t\t\t\t\t\tmin_count=1,\t # Ignores all words with a total frequency lower than this\n\t\t\t\t\t\tworkers=4)\t # Number of CPU cores to use for training the model\n\n# Training\nskipgram_model.train([tokenized_corpus], total_examples=1, epochs=10)\nskipgram_model.save(\"skipgram_model.model\")\nloaded_model = Word2Vec.load(\"skipgram_model.model\")\nvector_representation = loaded_model.wv['word']\nprint(\"Vector representation of 'word':\", vector_representation)\n</pre> !pip install gensim from gensim.models import Word2Vec from nltk.tokenize import word_tokenize import nltk nltk.download('punkt') # Download the tokenizer models if not already downloaded  sample = \"Word embeddings are dense vector representations of words.\" tokenized_corpus = word_tokenize(sample.lower()) # Lowercasing for consistency  skipgram_model = Word2Vec(sentences=[tokenized_corpus], \t\t\t\t\t\tvector_size=100, # Dimensionality of the word vectors \t\t\t\t\t\twindow=5,\t\t # Maximum distance between the current and predicted word within a sentence \t\t\t\t\t\tsg=1,\t\t\t # Skip-Gram model (1 for Skip-Gram, 0 for CBOW) \t\t\t\t\t\tmin_count=1,\t # Ignores all words with a total frequency lower than this \t\t\t\t\t\tworkers=4)\t # Number of CPU cores to use for training the model  # Training skipgram_model.train([tokenized_corpus], total_examples=1, epochs=10) skipgram_model.save(\"skipgram_model.model\") loaded_model = Word2Vec.load(\"skipgram_model.model\") vector_representation = loaded_model.wv['word'] print(\"Vector representation of 'word':\", vector_representation)  <pre>Requirement already satisfied: gensim in d:\\anaconda\\lib\\site-packages (4.3.0)\nRequirement already satisfied: FuzzyTM&gt;=0.4.0 in d:\\anaconda\\lib\\site-packages (from gensim) (2.0.5)\nRequirement already satisfied: numpy&gt;=1.18.5 in d:\\anaconda\\lib\\site-packages (from gensim) (1.23.5)\nRequirement already satisfied: scipy&gt;=1.7.0 in d:\\anaconda\\lib\\site-packages (from gensim) (1.10.0)\nRequirement already satisfied: smart-open&gt;=1.8.1 in d:\\anaconda\\lib\\site-packages (from gensim) (5.2.1)\nRequirement already satisfied: pyfume in d:\\anaconda\\lib\\site-packages (from FuzzyTM&gt;=0.4.0-&gt;gensim) (0.2.25)\nRequirement already satisfied: pandas in d:\\anaconda\\lib\\site-packages (from FuzzyTM&gt;=0.4.0-&gt;gensim) (1.5.3)\nRequirement already satisfied: python-dateutil&gt;=2.8.1 in d:\\anaconda\\lib\\site-packages (from pandas-&gt;FuzzyTM&gt;=0.4.0-&gt;gensim) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in d:\\anaconda\\lib\\site-packages (from pandas-&gt;FuzzyTM&gt;=0.4.0-&gt;gensim) (2022.7)\nRequirement already satisfied: fst-pso in d:\\anaconda\\lib\\site-packages (from pyfume-&gt;FuzzyTM&gt;=0.4.0-&gt;gensim) (1.8.1)\nRequirement already satisfied: simpful in d:\\anaconda\\lib\\site-packages (from pyfume-&gt;FuzzyTM&gt;=0.4.0-&gt;gensim) (2.11.0)\nRequirement already satisfied: six&gt;=1.5 in d:\\anaconda\\lib\\site-packages (from python-dateutil&gt;=2.8.1-&gt;pandas-&gt;FuzzyTM&gt;=0.4.0-&gt;gensim) (1.16.0)\nRequirement already satisfied: miniful in d:\\anaconda\\lib\\site-packages (from fst-pso-&gt;pyfume-&gt;FuzzyTM&gt;=0.4.0-&gt;gensim) (0.0.6)\n</pre> <pre>[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\zhongmeiqi\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n</pre> <pre>Vector representation of 'word': [-9.5800208e-03  8.9437785e-03  4.1664648e-03  9.2367809e-03\n  6.6457358e-03  2.9233587e-03  9.8055992e-03 -4.4231843e-03\n -6.8048164e-03  4.2256550e-03  3.7299085e-03 -5.6668529e-03\n  9.7035142e-03 -3.5551414e-03  9.5499391e-03  8.3657773e-04\n -6.3355025e-03 -1.9741615e-03 -7.3781307e-03 -2.9811086e-03\n  1.0425397e-03  9.4814906e-03  9.3598543e-03 -6.5986011e-03\n  3.4773252e-03  2.2767992e-03 -2.4910474e-03 -9.2290826e-03\n  1.0267317e-03 -8.1645092e-03  6.3240929e-03 -5.8001447e-03\n  5.5353874e-03  9.8330071e-03 -1.5987856e-04  4.5296676e-03\n -1.8086446e-03  7.3613892e-03  3.9419360e-03 -9.0095028e-03\n -2.3953868e-03  3.6261671e-03 -1.0080514e-04 -1.2024897e-03\n -1.0558038e-03 -1.6681013e-03  6.0541567e-04  4.1633579e-03\n -4.2531900e-03 -3.8336846e-03 -5.0755290e-05  2.6549282e-04\n -1.7014991e-04 -4.7843382e-03  4.3120929e-03 -2.1710952e-03\n  2.1056964e-03  6.6702347e-04  5.9686624e-03 -6.8418151e-03\n -6.8183104e-03 -4.4762432e-03  9.4359247e-03 -1.5930856e-03\n -9.4291316e-03 -5.4270827e-04 -4.4478951e-03  5.9980620e-03\n -9.5831212e-03  2.8602476e-03 -9.2544509e-03  1.2484600e-03\n  6.0004774e-03  7.4001122e-03 -7.6209377e-03 -6.0561695e-03\n -6.8399287e-03 -7.9184016e-03 -9.4984965e-03 -2.1255787e-03\n -8.3757477e-04 -7.2564054e-03  6.7876028e-03  1.1183097e-03\n  5.8291717e-03  1.4714618e-03  7.9081533e-04 -7.3718326e-03\n -2.1769912e-03  4.3199472e-03 -5.0856168e-03  1.1304744e-03\n  2.8835384e-03 -1.5386029e-03  9.9318363e-03  8.3507905e-03\n  2.4184163e-03  7.1170190e-03  5.8888551e-03 -5.5787875e-03]\n</pre> <p>In practice, the choice between CBOW and Skip-gram often depends on the specific characteristics of the data and the task at hand. CBOW might be preferred when training resources are limited, and capturing syntactic information is important. Skip-gram, on the other hand, might be chosen when semantic relationships and the representation of rare words are crucial.</p> <p>Pre-trained word embeddings are representations of words that are learned from large corpora and are made available for reuse in various natural language processing (NLP) tasks. These embeddings capture semantic relationships between words, allowing the model to understand similarities and relationships between different words in a meaningful way.</p> <p>GloVe is trained on global word co-occurrence statistics. It leverages the global context to create word embeddings that reflect the overall meaning of words based on their co-occurrence probabilities. this method, we take the corpus and iterate through it and get the co-occurrence of each word with other words in the corpus. We get a co-occurrence matrix through this. The words which occur next to each other get a value of 1, if they are one word apart then 1/2, if two words apart then 1/3 and so on.</p> <p>Let us take an example to understand how the matrix is created. We have a small corpus:</p> <p>Corpus:</p> <p>It is a nice evening.</p> <p>Good Evening!</p> <p>Is it a nice evening?</p> <p>| |it|is|a|nice|evening|good| |----------|----------|----------|----------|----------|----------|----------| |it|0| |is|1+1|0| |a|1/2+1|1+1/2|0| |nice|1/3+1/2|1/2+1/3|1+1|0| |evening|1/4+1/3|1/3+1/4|1/2+1/2|1+1|0| |good|0|0|0|0|1|0|</p> <p>The upper half of the matrix will be a reflection of the lower half. We can consider a window frame as well to calculate the co-occurrences by shifting the frame till the end of the corpus. This helps gather information about the context in which the word is used.</p> <p>Initially, the vectors for each word is assigned randomly. Then we take two pairs of vectors and see how close they are to each other in space. If they occur together more often or have a higher value in the co-occurrence matrix and are far apart in space then they are brought close to each other. If they are close to each other but are rarely or not frequently used together then they are moved further apart in space.</p> <p>After many iterations of the above process, we\u2019ll get a vector space representation that approximates the information from the co-occurrence matrix. The performance of GloVe is better than Word2Vec in terms of both semantic and syntactic capturing.</p> In\u00a0[\u00a0]: Copied! <pre>from gensim.models import KeyedVectors\nfrom gensim.downloader import load\n\nglove_model = load('glove-wiki-gigaword-50')\nword_pairs = [('learn', 'learning'), ('india', 'indian'), ('fame', 'famous')]\n\n# Compute similarity for each pair of words\nfor pair in word_pairs:\n\tsimilarity = glove_model.similarity(pair[0], pair[1])\n\tprint(f\"Similarity between '{pair[0]}' and '{pair[1]}' using GloVe: {similarity:.3f}\")\n</pre> from gensim.models import KeyedVectors from gensim.downloader import load  glove_model = load('glove-wiki-gigaword-50') word_pairs = [('learn', 'learning'), ('india', 'indian'), ('fame', 'famous')]  # Compute similarity for each pair of words for pair in word_pairs: \tsimilarity = glove_model.similarity(pair[0], pair[1]) \tprint(f\"Similarity between '{pair[0]}' and '{pair[1]}' using GloVe: {similarity:.3f}\")  <p>Output:</p> <p>Similarity between 'learn' and 'learning' using GloVe: 0.802</p> <p>Similarity between 'india' and 'indian' using GloVe: 0.865</p> <p>Similarity between 'fame' and 'famous' using GloVe: 0.589</p> <p>Developed by Facebook, FastText extends Word2Vec by representing words as bags of character n-grams. This approach is particularly useful for handling out-of-vocabulary words and capturing morphological variations.</p> In\u00a0[\u00a0]: Copied! <pre>import gensim.downloader as api\nfasttext_model = api.load(\"fasttext-wiki-news-subwords-300\") ## Load the pre-trained fastText model\n# Define word pairs to compute similarity for\nword_pairs = [('learn', 'learning'), ('india', 'indian'), ('fame', 'famous')]\n\n# Compute similarity for each pair of words\nfor pair in word_pairs:\n\tsimilarity = fasttext_model.similarity(pair[0], pair[1])\n\tprint(f\"Similarity between '{pair[0]}' and '{pair[1]}' using FastText: {similarity:.3f}\")\n</pre> import gensim.downloader as api fasttext_model = api.load(\"fasttext-wiki-news-subwords-300\") ## Load the pre-trained fastText model # Define word pairs to compute similarity for word_pairs = [('learn', 'learning'), ('india', 'indian'), ('fame', 'famous')]  # Compute similarity for each pair of words for pair in word_pairs: \tsimilarity = fasttext_model.similarity(pair[0], pair[1]) \tprint(f\"Similarity between '{pair[0]}' and '{pair[1]}' using FastText: {similarity:.3f}\")  <p>Output:</p> <p>Similarity between 'learn' and 'learning' using Word2Vec: 0.642</p> <p>Similarity between 'india' and 'indian' using Word2Vec: 0.708</p> <p>Similarity between 'fame' and 'famous' using Word2Vec: 0.519</p> <p>BERT is a transformer-based model that learns contextualized embeddings for words. It considers the entire context of a word by considering both left and right contexts, resulting in embeddings that capture rich contextual information.</p> In\u00a0[\u00a0]: Copied! <pre>from transformers import BertTokenizer, BertModel\nimport torch\n\n# Load pre-trained BERT model and tokenizer\nmodel_name = 'bert-base-uncased'\ntokenizer = BertTokenizer.from_pretrained(model_name)\nmodel = BertModel.from_pretrained(model_name)\n\nword_pairs = [('learn', 'learning'), ('india', 'indian'), ('fame', 'famous')]\n\n# Compute similarity for each pair of words\nfor pair in word_pairs:\n\ttokens = tokenizer(pair, return_tensors='pt')\n\twith torch.no_grad():\n\t\toutputs = model(**tokens)\n\t\n\t# Extract embeddings for the [CLS] token\n\tcls_embedding = outputs.last_hidden_state[:, 0, :]\n\n\tsimilarity = torch.nn.functional.cosine_similarity(cls_embedding[0], cls_embedding[1], dim=0)\n\t\n\tprint(f\"Similarity between '{pair[0]}' and '{pair[1]}' using BERT: {similarity:.3f}\")\n</pre> from transformers import BertTokenizer, BertModel import torch  # Load pre-trained BERT model and tokenizer model_name = 'bert-base-uncased' tokenizer = BertTokenizer.from_pretrained(model_name) model = BertModel.from_pretrained(model_name)  word_pairs = [('learn', 'learning'), ('india', 'indian'), ('fame', 'famous')]  # Compute similarity for each pair of words for pair in word_pairs: \ttokens = tokenizer(pair, return_tensors='pt') \twith torch.no_grad(): \t\toutputs = model(**tokens) \t \t# Extract embeddings for the [CLS] token \tcls_embedding = outputs.last_hidden_state[:, 0, :]  \tsimilarity = torch.nn.functional.cosine_similarity(cls_embedding[0], cls_embedding[1], dim=0) \t \tprint(f\"Similarity between '{pair[0]}' and '{pair[1]}' using BERT: {similarity:.3f}\")  <p>Output:</p> <p>Similarity between 'learn' and 'learning' using BERT: 0.930</p> <p>Similarity between 'india' and 'indian' using BERT: 0.957</p> <p>Similarity between 'fame' and 'famous' using BERT: 0.956</p> <ul> <li>You need to use the exact same pipeline during deploying your model as were used to create the training data for the word embedding. If you use a different tokenizer or different method of handling white space, punctuation etc. you might end up with incompatible inputs.</li> </ul> <ul> <li>Words in your input that doesn\u2019t have a pre-trained vector. Such words are known as Out of Vocabulary Word(oov). What you can do is replace those words with \u201cUNK\u201d which means unknown and then handle them separately.</li> </ul> <ul> <li>Dimension mis-match: Vectors can be of many lengths. If you train a model with vectors of length say 400 and then try to apply vectors of length 1000 at inference time, you will run into errors. So make sure to use the same dimensions throughout.</li> </ul> <ul> <li>It is much faster to train than hand build models like WordNet (which uses graph embeddings).</li> <li>Almost all modern NLP applications start with an embedding layer.</li> <li>It Stores an approximation of meaning.</li> </ul> <ul> <li>It can be memory intensive.</li> <li>It is corpus dependent. Any underlying bias will have an effect on your model.</li> <li>It cannot distinguish between homophones. Eg: brake/break, cell/sell, weather/whether etc.</li> </ul> <p>In conclusion, word embedding techniques such as TF-IDF, Word2Vec, and GloVe play a crucial role in natural language processing by representing words in a lower-dimensional space, capturing semantic and syntactic information.</p>"},{"location":"notebook/text-representation/#word-embedding","title":"Word embedding\u00b6","text":""},{"location":"notebook/text-representation/#discrete-representation","title":"Discrete representation\u00b6","text":""},{"location":"notebook/text-representation/#one-hot","title":"One-Hot\u00b6","text":""},{"location":"notebook/text-representation/#bag-of-word-bow","title":"Bag of Word (Bow)\u00b6","text":""},{"location":"notebook/text-representation/#term-frequency-inverse-document-frequency-tf-idf","title":"Term frequency-inverse document frequency (TF-IDF)\u00b6","text":""},{"location":"notebook/text-representation/#distributed-representation","title":"Distributed representation\u00b6","text":""},{"location":"notebook/text-representation/#word2vec","title":"Word2Vec\u00b6","text":""},{"location":"notebook/text-representation/#continuous-bag-of-wordscbow","title":"Continuous Bag of Words(CBOW)\u00b6","text":""},{"location":"notebook/text-representation/#skip-gram","title":"Skip-Gram\u00b6","text":""},{"location":"notebook/text-representation/#pretrained-word-embedding","title":"Pretrained Word-Embedding\u00b6","text":""},{"location":"notebook/text-representation/#glove","title":"GloVe\u00b6","text":""},{"location":"notebook/text-representation/#fasttext","title":"Fasttext\u00b6","text":""},{"location":"notebook/text-representation/#bert-bidirectional-encoder-representations-from-transformers","title":"BERT (Bidirectional Encoder Representations from Transformers)\u00b6","text":""},{"location":"notebook/text-representation/#considerations-for-deploying-word-embedding-models","title":"Considerations for Deploying Word Embedding Models\u00b6","text":""},{"location":"notebook/text-representation/#advantages-and-disadvantage-of-word-embeddings","title":"Advantages and Disadvantage of Word Embeddings\u00b6","text":""},{"location":"notebook/text-representation/#advantages","title":"Advantages\u00b6","text":""},{"location":"notebook/text-representation/#disadvantages","title":"Disadvantages\u00b6","text":""},{"location":"notebook/text-representation/#conclusion","title":"Conclusion\u00b6","text":""},{"location":"notebook/text-representation/#your-turn","title":"Your turn! \ud83d\ude80\u00b6","text":"<p>Assignment - News topic classification tasks</p>"},{"location":"notebook/text-representation/#acknowledgments","title":"Acknowledgments\u00b6","text":"<p>Thanks to GeeksforGeeks for creating the open-source project Word Embeddings in NLP.It inspire the majority of the content in this chapter.</p>"}]}